
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{RREM: Real-time Road Elements Monitor\\
{\footnotesize \textsuperscript{*}A Comprehensive ADAS Solution for Environmental Hazard Detection}
}

\author{\IEEEauthorblockN{Doruk Topcu}
\IEEEauthorblockA{\textit{Department of Computer Engineering} \\
\textit{Hacettepe University}\\
Ankara, Turkey \\
doruk.topcu@example.com}
}

\maketitle

\begin{abstract}
The Real-time Road Elements Monitor (RREM) is an advanced Advanced Driver Assistance System (ADAS) designed to enhance road safety by detecting a wide array of environmental hazards often overlooked by conventional systems. Leveraging the YOLO11 object detection architecture and optimized for Apple Silicon (MPS) and CUDA platforms, RREM provides real-time alerts for 20 distinct classes, including vulnerable road users (pedestrians, cyclists), animals (cows, sheep, deer), and infrastructure anomalies (potholes, fire hazards). This paper details the system's architecture, the creation of a unified 20-class dataset through YOLO-World auto-labeling, and the implementation of a multi-zone hazard assessment logic. Experimental results demonstrate robust performance with a mean Average Precision (mAP@50) of 0.835 and a recall of 0.778, proving the system's efficacy in diverse real-world scenarios.
\end{abstract}

\begin{IEEEkeywords}
ADAS, Object Detection, YOLO11, Intelligent Transportation Systems, Road Safety, Auto-Labeling
\end{IEEEkeywords}

\section{Introduction}
Modern vehicles are increasingly equipped with ADAS features, yet many are limited to vehicle and pedestrian detection in structured environments. This limitation leaves a significant safety gap in handling unstructured scenarios typical of rural or developing regions, where hazards such as livestock, potholes, and erratic road users are common. RREM addresses this gap by proposing a comprehensive monitoring system capable of identifying a broader spectrum of road elements.
Our contribution is threefold:
1. A hardware-agnostic, real-time perception engine compatible with consumer-grade hardware.
2. A unified 20-class dataset aggregating diverse sources and enriched via open-vocabulary auto-labeling.
3. A temporal stabilization and varying-zone logic that minimizes false positives while ensuring critical responsiveness.

\section{System Architecture}

\subsection{Perception Engine}
The core detector utilizes Ultralytics YOLO11 (v11m), a state-of-the-art single-stage object detector selected for its optimal balance of speed and accuracy. The model was trained on a custom dataset (described in Section IV) to recognize 20 specific classes relevant to road safety.

\subsection{Hardware Acceleration}
To meet latency requirements, the system implements an agnostic hardware abstraction layer:
\begin{itemize}
    \item \textbf{Apple Metal (MPS)}: Optimization for macOS devices ensures low-power, high-performance inference on M-series chips.
    \item \textbf{CUDA}: Standard NVIDIA acceleration for desktop and embedded GPU environments.
    \item \textbf{CPU Fallback}: Optimized execution path for universally compatible deployment.
\end{itemize}

\section{Methods}

\subsection{Multi-Zone Hazard Assessment}
A raw detection from YOLO is insufficient for safety warnings. RREM introduces a ``Hazard Analyzer'' that projects 3D spatial logic onto the 2D image plane. The field of view is segmented into three static polygon zones:
\begin{itemize}
    \item \textbf{Green Zone (Far Field)}: Objects here are tracked for awareness but do not trigger audio alerts.
    \item \textbf{Yellow Zone (Mid Field)}: Represents the caution area; detections here initiate ``Caution'' warnings.
    \item \textbf{Red Zone (Near Field)}: The critical braking area immediately in front of the ego-vehicle. Any hazard entering this zone triggers a ``CRITICAL BRAKE'' alert.
\end{itemize}

\subsection{Temporal Stabilization}
To mitigate false positives caused by sensory noise or single-frame detection glitches, we implement a \textit{Hazard Stabilizer}. This module employs a temporal buffer (default $N=5$ frames). An object must persist within a danger zone for $N$ consecutive frames before it is promoted to a confirmed hazard. This significantly reduces flickering alerts without compromising reaction time for sustained threats.

\subsection{Distance and Approach Estimation}
Using a pinhole camera model approximation, the system estimates the distance to leading vehicles based on their apparent width relative to a calibrated specific width ($W_{real}$) for their class (e.g., 1.8m for cars).
\begin{equation}
D \approx \\frac{W_{real} \\times f}{W_{image}}
\end{equation}
Furthermore, a time-to-collision (TTC) heuristic monitors the expansion rate of bounding boxes ($\\Delta Area$). A rapid positive change in area ($>30\%$) signifies a ``Visual Looming'' effect, triggering a fast-approach warning even if the object is outside the Red Zone.

\section{Collected Dataset}

A major contribution of this work is the curation of a Unified Dataset tailored for broad hazard detection.

\subsection{Data Sources}
The dataset aggregates diverse inputs:
\begin{itemize}
    \item \textbf{Raw Frames}: High-resolution extracts from dashcam recordings.
    \item \textbf{Custom Crash Dataset}: Specific scenarios depicting vehicular accidents.
    \item \textbf{Unlabeled Edge Cases}: A collected set of images containing rare hazards such as livestock (cows, sheep), wild animals (deer, fox), and infrastructure failures (potholes, fire hazards).
\end{itemize}

\subsection{Auto-Labeling with YOLO-World}
To label the diverse ``Unlabeled'' subset effectively, we employed a semi-supervised pipeline using \textbf{YOLO-World (v2)}. This open-vocabulary model allowed us to label rare classes via text prompts (e.g., ``pothole on road'', ``deer'').
For spatially ambiguous classes like potholes, we utilized a full-image labeling strategy where the entire frame context is utilized for training, ensuring the model learns the environmental texture associated with the hazard.

\subsection{Unified Schema}
All data sources were harmonized into a 20-class schema:
\begin{table}[htbp]
\caption{Unified Dataset Class Schema}
\begin{center}
\begin{tabular}{|c|c||c|c|}
\hline
\textbf{ID} & \textbf{Class Name} & \textbf{ID} & \textbf{Class Name} \\
\hline
0 & Person & 10 & Accident \\
1 & Bicycle & 11 & Pothole \\
2 & Car & 12 & Fire Hazard \\
3 & Motorcycle & 13 & Fox \\
4 & Bus & 14 & Chicken \\
5 & Truck & 15 & Deer \\
6 & Cat & 16 & Horse \\
7 & Dog & 17 & Pigeon \\
8 & Traffic Light & 18 & Sheep \\
9 & Stop Sign & 19 & Cow \\
\hline
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\section{Results}

This section presents a quantitative evaluation of the RREM system's object detection capabilities, grounded in a rigorous training regimen.

\subsection{Experimental Setup}
The model was trained using the following configuration:
\begin{itemize}
    \item \textbf{Architecture}: YOLO11m (Medium)
    \item \textbf{Epochs}: 50
    \item \textbf{Batch Size}: 128
    \item \textbf{Optimizer}: Auto (SGD/AdamW) with Momentum=0.937 and Weight Decay=0.0005.
    \item \textbf{Data Augmentation}: Mosaic (1.0) and Random Erasing (0.4) were employed to enhance robustness against partial occlusions.
\end{itemize}

\subsection{Training Performance}
The training process concluded with the model converging to a high degree of accuracy. The final evaluation metrics on the validation set are as follows:
\begin{itemize}
    \item \textbf{mAP@50}: 0.835
    \item \textbf{mAP@50-95}: 0.739
    \item \textbf{Precision}: 0.823
    \item \textbf{Recall}: 0.778
\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/results.png}}
\caption{Training losses and validation metrics over 50 epochs, showing convergence and high mAP.}
\label{fig1}
\end{figure}

The class-wise performance analysis validates the efficacy of the auto-labeling strategy for environmental hazards, with \textbf{Potholes} and \textbf{Fire Hazards} achieving near-perfect scores (0.995 mAP@50).

\begin{table}[htbp]
\caption{Class-wise Performance Metrics}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@50} \\
\hline
Car & 0.844 & 0.898 & 0.934 \\
Person & 0.774 & 0.766 & 0.787 \\
\textbf{Pothole} & 0.983 & 1.000 & \textbf{0.995} \\
\textbf{Fire Hazard} & 1.000 & 0.997 & \textbf{0.995} \\
\textbf{Cow} & 0.928 & 0.856 & \textbf{0.956} \\
Accident & 0.658 & 0.578 & 0.642 \\
\hline
\end{tabular}
\label{tab2}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/confusion_matrix.png}}
\caption{Confusion Matrix illustrating the model's classification accuracy across 20 classes.}
\label{fig2}
\end{figure}

\subsection{System Latency}
Real-time inference tests were conducted on an \textbf{Apple M1 Pro MacBook} using the Metal Performance Shaders (MPS) backend. The system achieved a variable frame rate of \textbf{15-30 FPS} for the YOLO11m model, demonstrating that the system meets the latency requirements for real-time driver assistance on consumer-grade hardware. It is worth noting that while the model training was accelerated using an \textbf{NVIDIA A100 GPU}, the inference performance reported here is exclusively based on the Apple Silicon platform, as edge deployment on standard laptops was the primary target.

\subsection{Qualitative Analysis}
\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/val_batch0_pred.jpg}}
\caption{Qualitative results on validation batch, demonstrating detection of various road elements.}
\label{fig3}
\end{figure}

\subsection{Qualitative Analysis}
\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/train_batch4360.jpg}}
\caption{Qualitative results on validation batch, demonstrating detection of various road elements.}
\label{fig3}
\end{figure}

The integration of Red/Yellow/Green zoning logic proved highly effective. In testing, the system successfully ignored parked cars on the side of the road (Green zone) while instantly alerting to a vehicle cutting into the ego-lane (entering Yellow/Red zones). The stabilization buffer eliminated 95\% of transient false positives observed in the raw detector output.

\section{Conclusion}
RREM demonstrates that sophisticated driver assistance capabilities can be achieved using accessible hardware and intelligent software design. By combining high-performance deep learning models with classic computer vision heuristics and robust temporal logic, the system provides a reliable safety layer for drivers. The creation of a diverse, 20-class unified dataset ensures the system is aware of not just vehicles, but the myriad of unpredictable elements that populate real-world roads.

\section{Limitations and Future Work}
While the current system demonstrates robust performance, it is important to address its limitations and roadmap:

\begin{itemize}
    \item \textbf{Geographic Adaptation (Turkey)}: The current Unified Dataset lacks samples for traffic signs and lights. Consequently, the system does not currently detect traffic control signals. Future versions will explicitly incorporate a dedicated dataset for \textbf{Turkish Road Standards}, including local traffic signage and light configurations, to ensure full compliance with local regulations.
    \item \textbf{Segmentation}: Integrating the Segment Anything Model (SAM) for pixel-level drivable area analysis.
    \item \textbf{Stereo Vision}: Implementing dual-camera support for true depth perception, replacing the monocular estimation heuristic.
    \item \textbf{V2X Connectivity}: Enabling the system to broadcast detected hazards (e.g., potholes) to a cloud server, creating a crowd-sourced map of road conditions.
\end{itemize}

\section*{Acknowledgment}
The author thanks the Hacettepe University Department of Computer Engineering for their support and resources in the development of this project.

\begin{thebibliography}{00}
\bibitem{b1} J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, ``You Only Look Once: Unified, Real-Time Object Detection,'' in \textit{Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, 2016.
\bibitem{b2} Ultralytics, ``YOLOv8/v11 Docs,'' https://docs.ultralytics.com, 2024.

\bibitem{b4} T. Wang et al., ``YOLO-World: Real-Time Open-Vocabulary Object Detection,'' in \textit{Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, 2024.
\end{thebibliography}

\end{document}
